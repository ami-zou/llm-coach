# setup_ollama.py - Setup script for Ollama with Chinese models
import requests
import subprocess
import time

def check_ollama_installed():
    """Check if Ollama is installed."""
    try:
        result = subprocess.run(["ollama", "--version"], capture_output=True, text=True)
        if result.returncode == 0:
            print(f"âœ… Ollamaå·²å®‰è£…: {result.stdout.strip()}")
            return True
        else:
            print("âŒ Ollamaæœªæ­£ç¡®å®‰è£…")
            return False
    except FileNotFoundError:
        print("âŒ Ollamaæœªå®‰è£…ã€‚è¯·è®¿é—® https://ollama.ai ä¸‹è½½å®‰è£…")
        return False

def start_ollama_server():
    """Start Ollama server if not running."""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=2)
        if response.status_code == 200:
            print("âœ… OllamaæœåŠ¡å·²åœ¨è¿è¡Œ")
            return True
    except:
        pass
    
    print("ğŸš€ å¯åŠ¨OllamaæœåŠ¡...")
    try:
        # Start Ollama server in background
        subprocess.Popen(["ollama", "serve"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        time.sleep(3)  # Wait for server to start
        
        # Check if it's running
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("âœ… OllamaæœåŠ¡å¯åŠ¨æˆåŠŸ")
            return True
        else:
            print("âŒ OllamaæœåŠ¡å¯åŠ¨å¤±è´¥")
            return False
    except Exception as e:
        print(f"âŒ å¯åŠ¨OllamaæœåŠ¡æ—¶å‡ºé”™: {e}")
        return False

def get_available_models():
    """Get list of available models."""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            return [model["name"] for model in models]
        return []
    except:
        return []

def pull_chinese_model():
    """Pull a good Chinese model."""
    chinese_models = [
        "qwen2.5:7b",  # Good Chinese model
        "deepseek-coder:6.7b",  # Good for coding and general tasks
        "llama3.2:3b",  # Smaller but capable
        "gemma2:9b",  # Google's model
        "deepseek-llm:7b" # Newest deepseek llm model
    ]
    
    print("ğŸ“¥ æ¨èçš„ä¸­æ–‡æ¨¡å‹:")
    for i, model in enumerate(chinese_models, 1):
        print(f"  {i}. {model}")
    
    print("\nğŸ’¡ å»ºè®®ä½¿ç”¨ 'qwen2.5:7b' æˆ– 'deepseek-coder:6.7b'")
    print("   è¿è¡Œä»¥ä¸‹å‘½ä»¤ä¸‹è½½æ¨¡å‹:")
    print("   ollama pull qwen2.5:7b")
    print("   æˆ–")
    print("   ollama pull deepseek-coder:6.7b")

def main():
    """Main setup function."""
    print("ğŸ”§ Ollama RAGç³»ç»Ÿè®¾ç½®")
    print("="*50)
    
    # Check if Ollama is installed
    if not check_ollama_installed():
        print("\nğŸ“¥ è¯·å…ˆå®‰è£…Ollama:")
        print("   macOS: brew install ollama")
        print("   æˆ–è®¿é—®: https://ollama.ai")
        return
    
    # Start Ollama server
    if not start_ollama_server():
        return
    
    # Get available models
    models = get_available_models()
    if models:
        print(f"\nğŸ“‹ å½“å‰å¯ç”¨æ¨¡å‹: {models}")
    else:
        print("\nğŸ“‹ æš‚æ— æ¨¡å‹ï¼Œéœ€è¦ä¸‹è½½")
    
    # Suggest Chinese models
    pull_chinese_model()
    
    print("\nâœ… è®¾ç½®å®Œæˆï¼")
    print("ğŸš€ ç°åœ¨å¯ä»¥è¿è¡Œ: python run_ollama_rag.py")

if __name__ == "__main__":
    main()
