# run_ollama_rag.py - RAG system using local Ollama
import requests
import json
from query_rag import retrieve_relevant_chunks

# Ollama configuration
OLLAMA_BASE_URL = "http://localhost:11434"
MODEL_NAME = "deepseek-llm:7b"  # You can change this to any model you have

def generate_answer_with_ollama(context_chunks, user_query):
    """Generate answer using local Ollama."""
    # Create context from retrieved chunks
    context = "\n\n".join([f"â€¢ {chunk}" for chunk in context_chunks])
    
    # Create prompt in Chinese
    prompt = f"""ä½ æ˜¯èŒä¸šæ•™ç»ƒQQï¼Œä¸“é—¨å¸®åŠ©è§£å†³äººç”Ÿé—®é¢˜ã€‚åŸºäºä»¥ä¸‹è®²åº§ç¬”è®°å†…å®¹ï¼Œè¯·ç»™å‡ºä¸“ä¸šå»ºè®®ï¼š

ã€ç›¸å…³ç¬”è®°å†…å®¹ã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{user_query}

ã€æ•™ç»ƒQQçš„ä¸“ä¸šå›ç­”ã€‘è¯·åŸºäºä»¥ä¸Šå†…å®¹ç»™å‡ºå…·ä½“ã€å®ç”¨çš„å»ºè®®ï¼š"""

    # Prepare request for Ollama
    payload = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.7,
            "top_p": 0.9,
            "max_tokens": 500
        }
    }
    
    try:
        # Send request to Ollama
        response = requests.post(
            f"{OLLAMA_BASE_URL}/api/generate",
            json=payload,
            timeout=60
        )
        response.raise_for_status()
        
        # Parse response
        result = response.json()
        return result.get("response", "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå›ç­”ã€‚").strip()
        
    except requests.exceptions.ConnectionError:
        return "âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ã€‚è¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œã€‚"
    except requests.exceptions.Timeout:
        return "âŒ è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•ã€‚"
    except Exception as e:
        return f"âŒ é”™è¯¯: {str(e)}"

def check_ollama_connection():
    """Check if Ollama is running and get available models."""
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            print(f"âœ… Ollamaè¿æ¥æˆåŠŸï¼")
            print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {[model['name'] for model in models]}")
            return True
        else:
            print(f"âŒ Ollamaå“åº”é”™è¯¯: {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print("âŒ æ— æ³•è¿æ¥åˆ°Ollamaã€‚è¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œï¼š")
        print("   ollama serve")
        return False
    except Exception as e:
        print(f"âŒ è¿æ¥Ollamaæ—¶å‡ºé”™: {e}")
        return False

def main():
    """Main function to run the RAG system."""
    print("ğŸš€ Starting Ollama RAG System...")
    
    # Check Ollama connection
    if not check_ollama_connection():
        return
    
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    print("\n" + "="*60)
    print("ğŸ’¬ æ¬¢è¿ä½¿ç”¨QQæ•™ç»ƒRAGç³»ç»Ÿï¼")
    print("ğŸ’¡ è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("="*60)
    
    while True:
        try:
            # Get user query
            query = input("\nğŸ’¬ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
            
            if query.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ å†è§ï¼")
                break
                
            if not query:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜")
                continue
            
            print(f"\nğŸ” æ­£åœ¨æœç´¢ç›¸å…³å†…å®¹...")
            
            # Retrieve relevant chunks
            chunks = retrieve_relevant_chunks(query, n_results=3)
            
            if not chunks:
                print("âŒ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
                continue
            
            print(f"ğŸ“š æ‰¾åˆ° {len(chunks)} ä¸ªç›¸å…³ç‰‡æ®µ")
            print("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­”...")
            
            # Generate answer
            answer = generate_answer_with_ollama(chunks, query)
            
            print(f"\nğŸ¤– QQæ•™ç»ƒçš„å»ºè®®ï¼š")
            print("="*60)
            print(answer)
            print("="*60)
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            continue

if __name__ == "__main__":
    main()
