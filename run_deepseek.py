import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
from query_rag import retrieve_relevant_chunks

# Load model once
MODEL_NAME = "deepseek-ai/deepseek-llm-7b-chat"

def load_model():
    """Load the model and tokenizer."""
    print("ğŸ¤– Loading DeepSeek model...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    
    # Set device
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    print(f"ğŸ–¥ï¸ Using device: {device}")
    
    # Load model with optimizations and offloading
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        dtype=torch.bfloat16,
        device_map="auto",
        low_cpu_mem_usage=True,
        trust_remote_code=True,
        offload_folder="./offload"
    )
    
    # Configure generation
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    return model, tokenizer

def generate_answer(context_chunks, user_query, model, tokenizer):
    """Generate answer using DeepSeek model."""
    # Create context from retrieved chunks
    context = "\n\n".join([f"â€¢ {chunk}" for chunk in context_chunks])
    
    # Create prompt in Chinese
    prompt = f"""ä½ æ˜¯èŒä¸šæ•™ç»ƒQQï¼Œä¸“é—¨å¸®åŠ©å¹´è½»å¥³æ€§è§£å†³æƒ…æ„Ÿå’Œäººç”Ÿé—®é¢˜ã€‚åŸºäºä»¥ä¸‹è®²åº§ç¬”è®°å†…å®¹ï¼Œè¯·ç»™å‡ºä¸“ä¸šå»ºè®®ï¼š

ã€ç›¸å…³ç¬”è®°å†…å®¹ã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{user_query}

ã€æ•™ç»ƒQQçš„ä¸“ä¸šå›ç­”ã€‘"""

    # Tokenize input
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
    
    # Generate response
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=300,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    # Decode response
    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    return response.strip()

def main():
    """Main function to run the RAG system."""
    print("ğŸš€ Starting DeepSeek RAG System...")
    
    # Load model
    model, tokenizer = load_model()
    
    print("\n" + "="*60)
    print("ğŸ’¬ æ¬¢è¿ä½¿ç”¨QQæ•™ç»ƒRAGç³»ç»Ÿï¼")
    print("ğŸ’¡ è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("="*60)
    
    while True:
        try:
            # Get user query
            query = input("\nğŸ’¬ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
            
            if query.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ å†è§ï¼")
                break
                
            if not query:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜")
                continue
            
            print(f"\nğŸ” æ­£åœ¨æœç´¢ç›¸å…³å†…å®¹...")
            
            # Retrieve relevant chunks
            chunks = retrieve_relevant_chunks(query, n_results=3)
            
            if not chunks:
                print("âŒ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
                continue
            
            print(f"ğŸ“š æ‰¾åˆ° {len(chunks)} ä¸ªç›¸å…³ç‰‡æ®µ")
            print("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­”...")
            
            # Generate answer
            answer = generate_answer(chunks, query, model, tokenizer)
            
            print(f"\nğŸ¤– QQæ•™ç»ƒçš„å»ºè®®ï¼š")
            print("="*60)
            print(answer)
            print("="*60)
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            continue

if __name__ == "__main__":
    main()

