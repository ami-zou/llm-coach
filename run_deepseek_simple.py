# run_deepseek_simple.py - Simplified version with smaller model
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from query_rag import retrieve_relevant_chunks

# Use a smaller model for testing
MODEL_NAME = "microsoft/DialoGPT-medium"  # Much smaller model for testing

def load_model():
    """Load the model and tokenizer."""
    print("ğŸ¤– Loading model...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
    
    # Configure generation
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    return model, tokenizer

def generate_answer(context_chunks, user_query, model, tokenizer):
    """Generate answer using the model."""
    # Create context from retrieved chunks
    context = "\n".join([f"- {chunk[:200]}..." for chunk in context_chunks[:3]])  # Limit context
    
    # Create prompt in Chinese
    prompt = f"""åŸºäºä»¥ä¸‹å†…å®¹å›ç­”é—®é¢˜ï¼š

å†…å®¹ï¼š{context}

é—®é¢˜ï¼š{user_query}

å›ç­”ï¼š"""

    # Tokenize input
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    # Generate response
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # Decode response
    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    return response.strip()

def main():
    """Main function to run the RAG system."""
    print("ğŸš€ Starting Simple RAG System...")
    
    # Load model
    model, tokenizer = load_model()
    
    print("\n" + "="*60)
    print("ğŸ’¬ æ¬¢è¿ä½¿ç”¨QQæ•™ç»ƒRAGç³»ç»Ÿï¼")
    print("ğŸ’¡ è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("="*60)
    
    while True:
        try:
            # Get user query
            query = input("\nğŸ’¬ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
            
            if query.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ å†è§ï¼")
                break
                
            if not query:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜")
                continue
            
            print(f"\nğŸ” æ­£åœ¨æœç´¢ç›¸å…³å†…å®¹...")
            
            # Retrieve relevant chunks
            chunks = retrieve_relevant_chunks(query, n_results=3)
            
            if not chunks:
                print("âŒ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
                continue
            
            print(f"ğŸ“š æ‰¾åˆ° {len(chunks)} ä¸ªç›¸å…³ç‰‡æ®µ")
            print("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­”...")
            
            # Generate answer
            answer = generate_answer(chunks, query, model, tokenizer)
            
            print(f"\nğŸ¤– QQæ•™ç»ƒçš„å»ºè®®ï¼š")
            print("="*60)
            print(answer)
            print("="*60)
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            continue

if __name__ == "__main__":
    main()
